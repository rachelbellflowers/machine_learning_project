---
title: "Student Performance"
author: "Rachel Bellflowers"
date: 'Last compiled: `r format(Sys.time(), "%b %d, %Y")`'
output: 
  bookdown::pdf_document2:
    fig_caption: yes
bibliography: [packages.bib]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, comment = NA, fig.align = "center")
library(magrittr)
library(broom)
library(dplyr)
library(caret)
library(knitr)
```


```{r include=FALSE}
# automatically create a bib database for R packages
# knitr::write_bib(c(
#   .packages(), 'bookdown', 'caret', 'knitr', 'rmarkdown', 'tidyverse', 'base'
# ), 'packages.bib')
```

```{r dataset, include = FALSE}
d1 <- read.table("student-mat.csv",sep=";",header = TRUE)
d2 <- read.table("student-por.csv",sep=";",header = TRUE)

d3 <- merge(d1, d2, by =c ("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet"))

d3 <- d3[ , 1:33]
```

```{r include = FALSE}

# count(d3, vars = sex)

d3[, 34] <- d3[, 14]
d3[, 35] <- d3[, 14]

d3[, 36] <- d3[, 16]
d3[, 37] <- d3[, 16]
d3[, 38] <- d3[, 16]


d3 <- d3 %<>% 
  mutate_if(is.factor, as.character)

colnames(d3)[14] <- "mother"
colnames(d3)[34] <- "father"
colnames(d3)[35] <- "other"

d3$mother <- ifelse(d3$mother == "mother", 1, 0)
d3$father <- ifelse(d3$father == "father", 1, 0)
d3$other <- ifelse(d3$other == "other", 1, 0)

colnames(d3)[16] <- "study_less_2"
colnames(d3)[36] <- "study_2_5"
colnames(d3)[37] <- "study_5_10"
colnames(d3)[38] <- "study_more_10"

d3$study_less_2 <- ifelse(d3$study_less_2 == 1, 1, 0)
d3$study_2_5 <- ifelse(d3$study_2_5 == 2, 1, 0)
d3$study_5_10 <- ifelse(d3$study_5_10 == 3, 1, 0)
d3$study_more_10 <- ifelse(d3$study_more_10 == 4, 1, 0)

colnames(d3)[31] <- "G1"
colnames(d3)[32] <- "G2"
colnames(d3)[33] <- "G3"

d3$school <- ifelse(d3$school == "MS", 1, 0)
d3$sex <- ifelse(d3$sex == "M", 1, 0)
d3$address <- ifelse(d3$address == "U", 1, 0)
d3$famsize <- ifelse(d3$famsize == "GT3", 1, 0)
d3$Pstatus <- ifelse(d3$Pstatus == "A", 1, 0)

d3$nursery <- ifelse(d3$nursery == "yes", 1, 0)
d3$internet <- ifelse(d3$internet == "yes", 1, 0)
d3$schoolsup.x <- ifelse(d3$schoolsup.x == "yes", 1, 0)
d3$famsup.x <- ifelse(d3$famsup.x == "yes", 1, 0)
d3$paid.x <- ifelse(d3$paid.x == "yes", 1, 0)
d3$activities.x <- ifelse(d3$activities.x == "yes", 1, 0)
d3$higher.x <- ifelse(d3$higher.x == "yes", 1, 0)
d3$romantic.x <- ifelse(d3$romantic.x == "yes", 1, 0)

final_grade <- d3[ , c(1:6, 12:14, 16, 18:23, 33:38)]
```

## Dataset Description {-}

This dataset examines how different factors affected 382 students' final grades for a math course. I used only the following variables from the dataset:

- **Dependent Variable**
  - `G3`: final grades for the class, ranging from 0-20 

- **Independent Variables**
   - `school`: whether they attended Gabriel Pereira or Mousinho da Silveira
   - `sex`: female or male
   - `age`: ranged from 15-22
   - `address`: whether they lived in a rural or urban area
   - `famsize`: whether their family had less than or equal to 3 members or greater than 3
   - `Pstatus`: parents separated or living together
   - `nursery`: whether they attended nursery school
   - `internet`: if they had Internet access at home
   - `guardian`: if they are taken care of by a mother, father, both, or other
   - `studytime`: How many hours they studied a week 
   - `famsup`: if their family supported their education
   - `paid`: if they were taking extra paid classes within the course subject (math)
   - `activities`: if they participated in extracurricular activities
   - `higher`: whether they intend to seek higher education
   - `romantic`: in a romantic relationship


## Linear Model {-}

First, I created a simple linear model using a training/test split of 50/50. Given that most the curve is centered on "0" for the residuals, the linear model appears to be a good predictor for final course grades.

```{r lin, echo = FALSE}
set.seed(42)

split <- round(nrow(final_grade) * 0.5)

# train
train <- final_grade[1:split, ]

# test
test <- final_grade[(split + 1):nrow(final_grade), ] 

lin1 <- lm(G3 ~., train)

# Predict on test
p <- augment(lin1, test)

ggplot(p, aes(.fitted, .resid)) +
  geom_point(color = "orange3") +
  geom_smooth(se = FALSE) +
  theme_bw() +
  labs(x = "Fitted Values", y = "Residuals", title = "Final Grades")
```

The plot in Figure \@ref(fig:lin) was created with `ggplot2` [@R-ggplot2].

---

## Random Forest {-}

I had doubts concerning the results of my initial linear model given that I used a 50/50 split. Therefore, I decided to run a random forest model. According to the graph, the less randomly selected predictors, the lower the RMSE. As the grading scale ranged from 1-20, a RMSE ranging from around 4.4 to around 4.7 suggests that this model does not serve as a good predictor of final course grades.

```{r label = tree, echo = FALSE}
set.seed(42)

mod_tree3 <- train(
    G3 ~ .,
    data = final_grade,
    method = "ranger"
)

plot(mod_tree3, main = "Final Grades")
```

The plot in Figure \@ref(fig:tree) was created with base R [@R-base].

---

## Classification Model {-}

To fit a classification model, I divided up grades into two categories: grades ranging from 1-10 and grades ranging from 11-20. I then used `method = glm` in the `train()` function. The green line, which has the highest lambda value, on average has a smaller ROC value than either the blue or pink lines.


```{r class, fig.width = 10, fig.height = 7, results = FALSE}

# Classification modeL; less than/equal to 10 or greater than or equal to 11

class_final <- final_grade

class_final$G3 <- ifelse(class_final$G3 <= 10, "no", "yes")

# Converting to factors

class_final$G3 <- as.factor(class_final$G3)

myControl <- trainControl(
  method = "cv",
  number = 10,
  summaryFunction = twoClassSummary,
  classProbs = TRUE,
  verboseIter = TRUE
)

set.seed(50)
class_mod <- train (
  G3 ~., 
  class_final,
  method = "glmnet",
  trControl = myControl
)

plot(class_mod)
```

The plot in Figure \@ref(fig:class) was created with base R [@R-base].

---

## Conclusion {-}

Although the first graph appeared to be the best out of the three, I believe that none of these models were good predictors of the outcome variable. The package suggested for creating ROC curves in the Datacamp exercise was not compatible with the version of RStudio Server we are using, so I am not sure if my last graph was calculated correctly. Additionally, I believe creating a 50/50 split for the first graph perhaps resulted in overfitting. In the future, a larger sample size with a greater age range would probably help with finding a greater effect.


# References{-}
